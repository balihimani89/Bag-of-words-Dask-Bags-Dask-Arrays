{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d22b4574-1114-42dc-a856-e92a3410c5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Row 0:\n",
      "['this', 'post', 'is', 'part', 'of', 'polyarchy', ',', 'an', 'independent', 'blog', 'produced', 'by', 'the', 'political', 'reform', 'program', 'at', 'new', 'america', ',', 'a', 'washington', 'think', 'tank', 'devoted', 'to', 'developing', 'new', 'ideas', 'and', 'new', 'voices', '.', 'imagine', 'you', 'are', 'an', 'otherwise', 'healthy', '30-something', 'who', 'starts', 'feeling', 'weird', '.', 'you', 'are', 'sometimes', 'short', 'of', 'breath', '.', 'you', 'get', 'migraines', '.', 'your', 'feet', 'start', 'to', 'swell', 'a', 'little', '.', 'but', 'otherwise', ',', 'everything', 'seems', 'fine', '.', 'you', 'go', 'to', 'the', 'doctor', '.', 'the', 'doctor', 'runs', 'some', 'tests', '.', 'she', 'tells', 'you', ',', 'it', \"'s\", 'probably', 'nothing', ',', 'but', 'these', 'could', 'be', 'signs', 'of', 'a', 'coming', 'heart', 'attack', '.', 'you', 'push', 'for', 'more', 'certainty', ',', 'but', 'the', 'doctor', 'tells', 'you', 'she', \"'s\", 'not', 'sure', '.', 'the', 'human', 'body', 'is', 'a', 'complex', 'system', '.', 'you', \"'re\", 'young', 'and', 'otherwise', 'pretty', 'healthy', '.', 'there', 'could', 'be', 'plenty', 'of', 'other', 'explanations', 'for', 'what', 'you', \"'re\", 'feeling', '.', 'but', 'it', 'is', 'a', 'little', 'worrying', '.', 'so', 'just', 'to', 'be', 'on', 'the', 'safe', 'side', ',', 'maybe', 'you', 'should', 'reduce', 'the', 'stress', 'in', 'your', 'life', 'and', 'eat', 'a', 'healthier', 'diet', '.', 'what', 'would', 'you', 'do', '?', 'if', 'you', \"'re\", 'a', 'sensible', 'person', ',', 'you', \"'d\", 'probably', 'err', 'on', 'the', 'side', 'of', 'precaution', '.', 'sure', ',', 'it', 'might', 'be', 'nothing', 'to', 'worry', 'about', ',', 'and', 'the', 'likelihood', 'of', 'a', 'heart', 'attack', 'in', 'your', '30s', 'might', 'be', 'low', '.', 'but', 'even', 'a', 'low', 'chance', 'is', 'a', 'low', 'chance', 'of', 'something', 'possibly', 'fatal', '.', 'why', 'take', 'a', 'chance', ',', 'especially', 'when', 'the', 'recommendations', '‚Äî', 'less', 'stress', ',', 'healthier', 'diet', '‚Äî', 'are', 'good', 'for', 'you', 'either', 'way', '?', 'i', 'offer', 'this', 'parable', 'as', 'a', 'way', 'of', 'thinking', 'about', 'the', 'debate', 'that', \"'s\", 'emerged', 'over', 'the', 'past', 'two', 'weeks', 'in', 'response', 'to', 'amanda', 'taub', \"'s\", 'new', 'york', 'times', 'article', 'profiling', 'new', 'findings', 'by', 'roberto', 'stefan', 'foa', 'and', 'yascha', 'mounk', '‚Äî', 'findings', 'that', 'raise', 'alarms', 'about', 'the', 'fact', 'that', 'younger', 'people', 'have', ',', 'over', 'time', ',', 'become', 'less', 'and', 'less', 'likely', 'to', 'say', 'in', 'surveys', 'that', 'it', 'is', '``', 'essential', \"''\", 'to', 'live', 'in', 'a', 'democracy', '.', 'rather', 'than', 'share', 'the', 'sense', 'of', 'alarm', ',', 'however', ',', 'several', 'critics', 'have', 'jumped', 'on', 'foa', 'and', 'mounk', 'for', 'misinterpreting', 'the', 'data', 'and', 'generating', 'unnecessary', 'panic', '.', 'political', 'scientist', 'erik', 'voeten', ',', 'for', 'example', ',', 'argued', 'that', 'their', 'analysis', 'is', 'misleading', ':', '``', 'the', 'article', 'by', 'mounk', 'and', 'foa', 'does', 'document', 'some', 'small', 'shifts', 'in', 'opinion', 'on', 'related', 'issues', '.', 'but', 'these', 'are', \"n't\", 'nearly', 'as', 'dramatic', 'as', 'the', 'new', 'york', 'times', 'graph', 'suggests', '.', \"''\", 'similarly', ',', 'wonkblog', \"'s\", 'jeff', 'guo', 'reanalyzed', 'the', 'data', 'and', 'argued', 'that', 'it', 'is', '``', 'far', 'less', 'alarming', 'than', 'it', 'seems', '.', \"''\", 'foa', 'and', 'mounk', 'have', 'responded', ',', 'drawing', 'on', 'more', 'analysis', 'from', 'their', 'forthcoming', 'journal', 'of', 'democracy', 'article', ',', 'which', 'also', 'documents', 'increasing', 'support', 'among', 'young', 'people', 'for', '``', 'a', 'strong', 'leader', \"''\", 'and', 'rising', 'support', 'for', 'extremism', '.', 'voeten', ',', 'however', ',', 'remains', 'unimpressed', ',', 'and', 'now', 'has', 'more', 'charts', 'here', 'suggesting', 'the', 'shifts', 'are', 'far', 'less', 'significant', 'than', 'foa', 'and', 'mounk', 'make', 'them', 'out', 'to', 'be', '.', '``', 'and', ',', \"''\", 'he', 'argues', ',', '``', 'it', \"'s\", 'dangerous', 'too', 'to', 'tell', 'the', 'world', 'that', 'people', 'are', 'now', 'ready', 'to', 'accept', 'nondemocratic', 'governance', '.', \"''\", 'for', 'those', 'who', 'want', 'to', 'argue', 'over', 'how', 'to', 'interpret', 'the', 'data', ',', 'you', 'should', 'follow', 'the', 'hyperlinks', 'above', '.', 'there', 'are', 'very', 'reasonable', 'points', 'of', 'disagreement', '.', 'i', 'do', \"n't\", 'have', 'much', 'to', 'add', 'to', 'that', 'debate', 'here', ',', 'other', 'than', 'to', 'observe', 'that', 'it', \"'s\", 'very', 'rare', 'that', 'data', 'is', 'unambiguous', 'about', 'important', 'societal', 'shifts', 'before', 'those', 'shifts', 'actually', 'occur', '.', 'when', 'the', 'data', 'is', 'unambiguous', ',', 'it', 'is', 'almost', 'always', 'too', 'late', 'to', 'do', 'anything', '.', 'the', 'only', 'sure', 'sign', 'of', 'having', 'a', 'heart', 'attack', 'is', ',', 'well', ',', 'having', 'a', 'heart', 'attack', '.', 'similarly', ',', 'the', 'only', 'sure', 'sign', 'of', 'a', 'democratic', 'collapse', 'is', ',', 'well', ',', 'a', 'democratic', 'collapse', '.', 'and', 'whatever', 'you', 'think', 'of', 'the', 'data', 'analysis', ',', 'there', 'is', 'also', 'a', 'mounting', 'series', 'of', 'actual', 'real-world', 'election', 'results', 'that', 'are', 'hard', 'to', 'explain', 'if', 'support', 'for', 'liberal', 'democracy', 'is', 'thriving', '.', 'in', 'deciding', 'how', 'seriously', 'to', 'take', 'these', 'findings', ',', 'it', \"'s\", 'also', 'worth', 'asking', 'what', 'we', 'would', 'do', 'differently', 'if', 'we', 'took', 'foa', 'and', 'mounk', \"'s\", 'findings', 'seriously', '.', 'how', 'would', 'we', 'collectively', 'respond', '?', 'and', 'what', 'would', 'be', 'the', 'consequences', '?', 'for', 'one', ',', 'we', \"'d\", 'probably', 'invest', 'in', 'a', 'lot', 'more', 'civic', 'education', ',', 'so', 'that', 'the', 'next', 'generation', 'learns', 'the', 'basics', 'of', 'liberal', 'democracy', 'and', 'understands', 'why', 'it', \"'s\", 'a', 'better', 'system', 'than', 'authoritarian', 'rule', '.', 'this', 'seems', 'like', 'a', 'good', 'idea', 'regardless', '.', 'similarly', ',', 'we', 'might', 'collectively', 'invest', 'considerable', 'resources', 'in', 'making', 'a', 'strong', 'public', 'case', 'for', 'liberal', 'democracy', '.', 'we', 'might', 'also', 'try', 'to', 'figure', 'out', 'ways', 'to', 'make', 'our', 'public', 'institutions', 'do', 'more', 'outreach', 'to', 'citizens', 'to', 'make', 'sure', 'they', 'feel', 'engaged', 'in', 'their', 'democracy', ',', 'and', 'think', 'hard', 'about', 'building', 'up', 'intermediary', 'institutions', 'that', 'help', 'people', 'feel', 'as', 'though', 'their', 'voices', 'are', 'represented', 'and', 'taken', 'seriously', '.', 'again', ',', 'these', 'seem', 'like', 'things', 'we', 'should', 'be', 'doing', 'regardless', ',', 'like', 'reducing', 'stress', 'or', 'improving', 'the', 'health', 'of', 'our', 'diets', '.', 'in', 'my', 'heart', 'attack', 'parable', ',', 'if', 'the', 'doctor', 'had', 'told', 'you', 'that', 'the', 'only', 'way', 'to', 'prevent', 'a', 'future', 'heart', 'attack', 'would', 'be', 'to', 'give', 'up', 'your', 'job', 'and', 'your', 'social', 'life', 'and', 'spend', 'the', 'next', 'year', 'on', 'strict', 'bed', 'rest', 'eating', 'only', 'kale', 'and', 'chia', 'seed', 'salads', ',', 'you', 'might', 'want', 'to', 'be', 'a', 'little', 'more', 'certain', 'that', 'you', 'really', 'were', 'at', 'high', 'risk', 'for', 'a', 'heart', 'attack', '.', 'after', 'all', ',', 'taking', 'the', 'risk', 'seriously', 'would', 'impose', 'a', 'heavy', 'cost', 'on', 'you', '.', 'other', 'recent', 'crises', 'offer', 'some', 'examples', 'of', 'cases', 'where', 'key', 'decision-makers', 'did', 'ignore', 'warning', 'signs', ',', 'because', 'taking', 'those', 'signs', 'seriously', 'would', 'have', 'imposed', 'significant', 'costs', 'on', 'them', '.', 'for', 'example', ',', 'in', 'the', 'housing', 'bubble', 'of', 'the', 'mid-2000s', ',', 'warnings', 'were', 'ignored', 'because', 'the', 'financial', 'industry', 'had', 'staked', 'considerable', 'investments', 'and', 'product', 'lines', 'on', 'the', 'myth', 'that', 'housing', 'values', 'would', 'go', 'up', 'forever', '.', 'to', 'admit', 'that', 'housing', 'was', 'overvalued', 'and', 'that', 'securitized', 'mortgages', 'were', 'riskier', 'than', 'advertised', 'would', 'have', 'cost', 'investment', 'banks', 'dearly', '.', 'but', 'eventually', ',', 'reality', 'caught', 'up', 'with', 'them', ',', 'and', 'the', 'resulting', 'damage', 'was', 'far', 'worse', 'than', 'it', 'would', 'have', 'been', 'if', 'we', 'had', 'paid', 'attention', 'to', 'the', 'early', 'warning', 'signs', '.', 'similarly', ',', 'many', 'carbon-intensive', 'industries', 'and', 'fossil', 'fuel', 'producers', 'pushed', 'back', 'on', 'findings', 'of', 'climate', 'change', 'because', 'taking', 'those', 'findings', 'seriously', 'would', 'force', 'significant', 'changes', 'in', 'their', 'industries', '.', 'as', 'a', 'result', ',', 'these', 'industries', 'funded', 'doubt', 'and', 'uncertainty', '.', 'the', 'problem', 'has', 'since', 'gotten', 'much', 'worse', ',', 'and', 'it', 'has', 'become', 'harder', 'to', 'take', 'effective', 'action', '.', 'the', 'early', 'scientists', 'may', 'have', 'been', 'alarmist', '.', 'but', 'we', \"'d\", 'be', 'in', 'much', 'better', 'shape', 'if', 'we', 'had', 'listened', 'to', 'them', '.', 'i', 'would', 'be', 'more', 'comforted', 'if', 'i', 'could', 'be', 'certain', 'that', 'voeten', 'is', 'right', 'and', 'foa', 'and', 'mounk', 'are', 'wrong', '.', 'maybe', 'there', 'is', 'indeed', 'nothing', 'to', 'worry', 'about', '.', 'but', 'given', 'the', 'risks', ',', 'as', 'well', 'as', 'the', 'recent', 'string', 'of', 'election', 'results', ',', 'i', \"'d\", 'rather', 'err', 'on', 'the', 'side', 'of', 'caution', '.', 'like', 'the', 'threat', 'of', 'a', 'heart', 'attack', ',', 'the', 'threat', 'of', 'autocracy', 'or', 'military', 'rule', 'replacing', 'liberal', 'democracy', 'is', 'pretty', 'serious', ',', 'and', 'very', 'difficult', 'to', 'recover', 'from', '.', 'i', 'do', \"n't\", 'want', 'to', 'take', 'a', 'chance', '.', 'especially', 'when', 'the', 'preventive', 'medicine', 'consists', 'of', 'things', 'we', 'should', 'probably', 'be', 'doing', 'anyway', '.']\n",
      "Type: <class 'list'>\n",
      "\n",
      "Row 1:\n",
      "['davos', ',', 'switzerland', '(', 'reuters', ')', '-', 'u.s.', 'president', 'donald', 'trump', 'denied', 'a', 'report', 'on', 'friday', 'that', 'he', 'had', 'ordered', 'special', 'counsel', 'robert', 'mueller', 'fired', 'last', 'june', ',', 'calling', 'it', '‚Äú', 'fake', 'news', '‚Äù', '.', 'the', 'new', 'york', 'times', 'reported', 'on', 'thursday', 'that', 'trump', 'backed', 'down', 'from', 'his', 'order', 'after', 'the', 'white', 'house', 'counsel', 'threatened', 'to', 'resign', 'rather', 'than', 'follow', 'his', 'directive', ',', 'citing', 'four', 'people', 'told', 'of', 'the', 'matter', '.', '‚Äú', 'fake', 'news', ',', 'folks', ',', 'fake', 'news', ',', '‚Äù', 'trump', 'told', 'reporters', 'in', 'davos', ',', 'when', 'asked', 'about', 'the', 'report', '.', 'reporting', 'by', 'steve', 'holland']\n",
      "Type: <class 'list'>\n",
      "\n",
      "Row 2:\n",
      "['paris', '(', 'reuters', ')', '-', 'former', 'french', 'president', 'nicolas', 'sarkozy', 'published', 'a', 'new', 'memoir', 'on', 'thursday', 'but', 'was', 'quick', 'to', 'dismiss', 'speculation', 'he', 'might', 'return', 'to', 'politics', 'to', 'rescue', 'his', 'old', 'center-right', 'party', ',', 'which', 'has', 'shed', 'support', 'since', 'president', 'emmanuel', 'macron', 'stormed', 'to', 'power', '.', 'entitled', '‚Äú', 'passions', '‚Äù', ',', 'the', 'autobiography', 'documents', 'sarkozy', '‚Äô', 's', 'rise', 'to', 'power', ',', 'while', 'hurling', 'barbs', 'at', 'some', 'of', 'his', 'former', 'allies', ',', 'including', 'his', 'prime', 'minister', 'francois', 'fillon', '.', 'during', 'his', 'time', 'in', 'office', 'from', '2007-2012', ',', 'sarkozy', 'earned', 'the', 'nickname', '‚Äú', 'president', 'bling-bling', '‚Äù', 'for', 'his', 'brash', 'style', 'and', 'close', 'association', 'with', 'tycoons', 'and', 'celebrities', '.', 'while', 'at', 'the', 'elysee', ',', 'he', 'met', 'and', 'married', 'former', 'model', 'carla', 'bruni', '.', '‚Äú', 'passions', '‚Äù', 'is', 'released', 'as', 'france', '‚Äô', 's', 'center-right', 'fights', 'for', 'survival', 'two', 'years', 'after', 'president', 'emmanuel', 'macron', '‚Äô', 's', 'election', 'win', 'dynamited', 'the', 'political', 'landscape', '.', 'sarkozy', '‚Äô', 's', 'old', 'party', ',', 'les', 'republicains', ',', 'is', 'locked', 'in', 'infighting', 'and', 'struggling', 'to', 'find', 'direction', ',', 'squeezed', 'by', 'macron', '‚Äô', 's', 'centrist', 'party', 'and', 'the', 'far', 'right', '.', '‚Äú', 'the', 'book', 'is', 'a', 'political', 'one', ',', 'it', 'does', 'not', 'announce', 'my', 'return', ',', '‚Äù', 'sarkozy', 'told', 'magazine', 'le', 'point', '.', '‚Äú', 'i', 'neither', 'want', 'nor', 'can', 'get', 'involved', 'in', 'a', 'partisan', 'debate', '.', 'that', 'would', 'sew', 'confusion', 'and', 'division', '.', 'there', 'is', 'no', 'need', 'for', 'that', '.', 'it', 'would', 'be', 'misplaced.', '‚Äù', 'nonetheless', ',', 'each', 'new', 'appearance', 'by', 'the', 'last', 'unifying', 'figure', 'of', 'the', 'french', 'right', 'is', 'closely', 'followed', '-', 'fearfully', 'by', 'those', 'wary', 'of', 'the', 'return', 'of', 'a', 'big-name', 'rival', ',', 'and', 'hopefully', 'by', 'those', 'who', 'long', 'for', 'sarkozy', '‚Äô', 's', 'political', 'reincarnation', '.', 'sarkozy', 'has', 'retired', 'twice', 'from', 'politics', ':', 'the', 'first', 'time', 'after', 'he', 'lost', 'his', 're-election', 'bid', 'in', '2012.', 'he', 'returned', 'to', 'lead', 'his', 'party', 'in', '2014', 'only', 'to', 'quit', 'again', 'after', 'he', 'failed', 'to', 'win', 'his', 'party', '‚Äô', 's', 'ticket', 'to', 'run', 'for', 'president', 'in', '2017.', 'les', 'republicains', 'are', 'again', 'without', 'a', 'leader', 'after', 'laurent', 'wauquiez', 'stepped', 'down', 'following', 'the', 'party', '‚Äô', 's', 'dismal', 'showing', 'in', 'may', '‚Äô', 's', 'european', 'election', '.', 'even', 'if', 'sarkozy', 'were', 'to', 'return', 'to', 'frontline', 'politics', ',', 'he', 'would', 'struggle', 'to', 'put', 'to', 'one', 'side', 'multiple', 'investigations', ',', 'including', 'into', 'the', 'financing', 'of', 'his', 're-election', 'bid', '.', 'this', 'month', 'he', 'learnt', 'he', 'must', 'stand', 'trial', 'on', 'charges', 'of', 'corruption', 'and', 'influence', 'peddling', 'in', 'relation', 'to', 'allegations', 'that', 'he', 'offered', 'a', 'judge', 'a', 'promotion', 'in', 'return', 'for', 'information', 'on', 'a', 'parallel', 'investigation', '.', 'sarkozy', 'has', 'denied', 'any', 'wrongdoing', '.', 'reporting', 'by', 'simon', 'carraud', ';', 'writing', 'by', 'richard', 'lough', ';', 'editing', 'by', 'luke', 'baker', 'and', 'hugh', 'lawson']\n",
      "Type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import ast\n",
    "from nltk.tokenize import word_tokenize\n",
    "from contractions import contractions_dict  # your custom contraction dictionary\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ========================\n",
    "# üìÇ Load CSV (adjust path\n",
    "\n",
    "df = dd.read_csv(\n",
    "    \"all-the-news-2-1-cleaned.csv\",\n",
    "    usecols=['article'],  # or any subset you actually need\n",
    "    dtype={'article': 'object'},\n",
    "    on_bad_lines='skip',\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# üî§ Normalize text\n",
    "# ========================\n",
    "def normalize_text(x):\n",
    "    if pd.isna(x) or x == '':\n",
    "        return ''\n",
    "    return x.lower()\n",
    "\n",
    "normalized_reviews = df['article'].map_partitions(\n",
    "    lambda col: col.map(normalize_text),\n",
    "    meta=pd.Series(dtype=object)\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# üß† Tokenize safely\n",
    "# ========================\n",
    "def text_tokenization(x):\n",
    "    try:\n",
    "        if pd.isna(x) or x.strip() == '':\n",
    "            return []\n",
    "        return word_tokenize(x)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "tokenized = normalized_reviews.map_partitions(\n",
    "    lambda col: col.map(text_tokenization),\n",
    "    meta=pd.Series(dtype=object)\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# üõ†Ô∏è Fix stringified lists (if needed)\n",
    "# ========================\n",
    "def safe_eval(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x) if isinstance(x, str) else x\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "tokenized = tokenized.map_partitions(\n",
    "    lambda col: col.map(safe_eval),\n",
    "    meta=pd.Series(dtype=object)\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# üîß Expand contractions\n",
    "# ========================\n",
    "def expand_token(token):\n",
    "    return contractions_dict.get(token, token)\n",
    "\n",
    "def expand_contractions(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        expanded = []\n",
    "        for token in tokens:\n",
    "            expanded.extend(expand_token(token).split())\n",
    "        return expanded\n",
    "    return []\n",
    "\n",
    "contracted_reviews = tokenized.map_partitions(\n",
    "    lambda col: col.map(expand_contractions),\n",
    "    meta=pd.Series(dtype=object)\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# ‚úÖ Inspect sample\n",
    "# ========================\n",
    "sample = contracted_reviews.head(3)\n",
    "for i, row in enumerate(sample):\n",
    "    print(f\"\\nRow {i}:\\n{row}\\nType: {type(row)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8afea9be-cc0b-4d00-b063-93d078b12d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'^@[a-zA-z0-9]|^#[a-zA-Z0-9]|\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*|\\W+|\\d+|<(\"[^\"]*\"|\\'[^\\']*\\'|[^\\'\">])*>|_+|[^\\u0000-\\u007f]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd834afa-80df-4356-9f3b-7358611ca95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from itertools import filterfalse\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac8f596-30bd-4976-8528-5016a83e49d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [this, post, is, part, of, polyarchy, an, inde...\n",
       "1    [davos, switzerland, reuters, president, donal...\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def waste_word_or_not(token):\n",
    "    return bool(re.search(regex, token))\n",
    "\n",
    "# Apply filter to a tokenized list\n",
    "def filter_waste_words(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        return list(filterfalse(waste_word_or_not, tokens))\n",
    "    return []\n",
    "\n",
    "# Apply over contracted tokenized reviews\n",
    "filtered_reviews = contracted_reviews.map_partitions(\n",
    "    lambda col: col.map(filter_waste_words),\n",
    "    meta=pd.Series(dtype=object)\n",
    ")\n",
    "\n",
    "# Preview\n",
    "filtered_reviews.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d7096d-94b4-4f88-b907-dcb93fe3de90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [post, polyarchy, independent, blog, produced,...\n",
       "1    [davos, switzerland, reuters, president, donal...\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        return [re.split(regex, x)[0] for x in tokens]\n",
    "    return []\n",
    "# 1. Split on regex\n",
    "filtered_reviews = filtered_reviews.map_partitions(\n",
    "    lambda col: col.map(split),\n",
    "    meta=pd.Series(dtype=object)\n",
    ")\n",
    "en_stop_words = set(stopwords.words('english')).union(STOP_WORDS)\n",
    "def is_stopword(token):\n",
    "    return not (\n",
    "        token in en_stop_words or \n",
    "        re.search(r'\\b\\w\\b|[^\\u0000-\\u007f]+|_+|\\W+', token)\n",
    "    )\n",
    "\n",
    "def stopwords_removal(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        return list(filter(is_stopword, tokens))\n",
    "    return []\n",
    "\n",
    "# 2. Remove stopwords and noise\n",
    "without_stopwords_reviews = filtered_reviews.map_partitions(\n",
    "    lambda col: col.map(stopwords_removal),\n",
    "    meta=pd.Series(dtype=object)\n",
    ")\n",
    "without_stopwords_reviews.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9286f9-4f9e-4ec9-8628-a26ae5f60f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<dask_expr.expr.Scalar: expr=MapPartitions(lambda).size(), dtype=int32>,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_stopwords_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f0ffdb-273d-41b5-a30d-a97276b8dd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(post, n), (polyarchy, n), (independent, a), ...\n",
       "1    [(davos, n), (switzerland, n), (reuters, n), (...\n",
       "Name: x, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pos_helpers import process_partition\n",
    "tagged_reviews = without_stopwords_reviews.map_partitions(process_partition, meta=('x', 'object'))\n",
    "tagged_reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b00d7e-c0bf-42fd-8c96-0ae042acb620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [post, polyarchy, independent, blog, produce, ...\n",
      "1    [davos, switzerland, reuters, president, donal...\n",
      "Name: x, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Instantiate lemmatizer at top-level (important for Dask)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def token_lemmatization(token_pos_tuple):\n",
    "    if token_pos_tuple is None or len(token_pos_tuple) < 2:\n",
    "        return \"\"\n",
    "    return lemmatizer.lemmatize(word=token_pos_tuple[0], pos=token_pos_tuple[1])\n",
    "\n",
    "def lemmatization(review):\n",
    "    if isinstance(review, list) and len(review) > 0:\n",
    "        return list(map(token_lemmatization, review))\n",
    "    return [\"\"]\n",
    "\n",
    "def process_lemmatization_partition(partition_series):\n",
    "    return partition_series.map(lemmatization)\n",
    "\n",
    "# Apply in Dask using named function (not lambda)\n",
    "lemmatized_reviews = tagged_reviews.map_partitions(\n",
    "    process_lemmatization_partition,\n",
    "    meta=('x', 'object')\n",
    ")\n",
    "\n",
    "# View result\n",
    "print(lemmatized_reviews.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c385ef4-d5c5-4807-884d-c0eca19d12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_tokens = lemmatized_reviews  # This is a dask Series\n",
    "import dask.bag as db\n",
    "\n",
    "# Convert series to bag\n",
    "token_bag = lemmatized_reviews.to_bag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "462417a4-e2a1-4966-9ef2-f749eeee97b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['say', 'like']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count(accumulator,element):\n",
    "    return accumulator + 1\n",
    "def combine(total_1,total_2):\n",
    "    return total_1 + total_2\n",
    "from dask.distributed import Client\n",
    "client = Client(processes=None)\n",
    "token_counts = token_bag.flatten().foldby(\n",
    "    key=lambda x: x,      # Group by token itself\n",
    "    binop=count,          # Count each occurrence\n",
    "    initial=0,            # Start from 0\n",
    "    combine=combine,      # Combine results from partitions\n",
    "    combine_initial=0     # Start from 0 when combining\n",
    ").compute()\n",
    "\n",
    "top_tokens = sorted(token_counts,key=lambda x:x[1],reverse=True)\n",
    "top_100_tokens = list(map(lambda x:x[0],top_tokens[:100]))\n",
    "top_100_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c4514cd-417a-47e6-a1bb-fbc5b1ba2042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, ...\n",
       "1    [0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def extract_bow_vector(review):\n",
    "    one_hot_encoded_bow_vector = np.where(np.isin(top_100_tokens,review),1,0)\n",
    "    review = one_hot_encoded_bow_vector\n",
    "    return review\n",
    "model_data = lemmatized_reviews.map_partitions(\n",
    "    lambda col: col.map(extract_bow_vector),\n",
    "    meta=pd.Series(dtype=object))\n",
    "model_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7b2ff6a-75da-457f-8be7-d8b223f78ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 22:03:07,153 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:62121 (pid=13904) exceeded 95% memory budget. Restarting...\n",
      "2025-08-08 22:03:07,893 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:62121' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('partition_elements_stacker-aggregate-b574a2d945351dee1dc425428ec07cad0', 0), ('partition_elements_stacker-aggregate-b574a2d945351dee1dc425428ec07cad0', 2)} (stimulus_id='handle-worker-cleanup-1754670787.882664')\n",
      "2025-08-08 22:03:08,620 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-08-08 23:09:34,877 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:62119 (pid=4040) exceeded 95% memory budget. Restarting...\n",
      "2025-08-08 23:09:35,033 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:62119 (pid=4040) is slow to terminate; trying again\n",
      "2025-08-08 23:09:35,095 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:62119 (pid=4040) is slow to terminate; trying again\n",
      "2025-08-08 23:09:35,173 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:62119 (pid=4040) is slow to terminate; trying again\n",
      "2025-08-08 23:09:35,298 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:62119' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('partition_elements_stacker-aggregate-b574a2d945351dee1dc425428ec07cad0', 0), ('partition_elements_stacker-aggregate-b574a2d945351dee1dc425428ec07cad0', 2)} (stimulus_id='handle-worker-cleanup-1754674775.2814314')\n",
      "2025-08-08 23:09:36,127 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-08-09 00:14:20,279 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:50115 (pid=7880) exceeded 95% memory budget. Restarting...\n",
      "2025-08-09 00:14:20,482 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:50115 (pid=7880) is slow to terminate; trying again\n",
      "2025-08-09 00:14:20,591 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:50115 (pid=7880) is slow to terminate; trying again\n",
      "2025-08-09 00:14:20,685 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:50115 (pid=7880) is slow to terminate; trying again\n",
      "2025-08-09 00:14:20,779 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:50115 (pid=7880) is slow to terminate; trying again\n",
      "2025-08-09 00:14:20,904 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:50115 (pid=7880) is slow to terminate; trying again\n",
      "2025-08-09 00:14:20,966 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:50115 (pid=7880) is slow to terminate; trying again\n",
      "2025-08-09 00:14:21,091 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:50115 (pid=7880) is slow to terminate; trying again\n",
      "2025-08-09 00:14:21,138 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:50115' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('partition_elements_stacker-aggregate-b574a2d945351dee1dc425428ec07cad0', 0), ('partition_elements_stacker-aggregate-b574a2d945351dee1dc425428ec07cad0', 2)} (stimulus_id='handle-worker-cleanup-1754678661.1390686')\n",
      "2025-08-09 00:14:21,607 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-08-09 01:18:16,524 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:49188 (pid=9980) exceeded 95% memory budget. Restarting...\n",
      "2025-08-09 01:18:16,665 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:49188 (pid=9980) is slow to terminate; trying again\n",
      "2025-08-09 01:18:16,727 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:49188 (pid=9980) is slow to terminate; trying again\n",
      "2025-08-09 01:18:16,821 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:49188 (pid=9980) is slow to terminate; trying again\n",
      "2025-08-09 01:18:16,930 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:49188 (pid=9980) is slow to terminate; trying again\n",
      "2025-08-09 01:18:17,024 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:49188 (pid=9980) is slow to terminate; trying again\n",
      "2025-08-09 01:18:17,118 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:49188 (pid=9980) is slow to terminate; trying again\n",
      "2025-08-09 01:18:17,196 - distributed.scheduler - ERROR - Task partition_elements_stacker-aggregate-finalize-hlgfinalizecompute-22ff46393343492ebf8fd4684d76bcb2 marked as failed because 4 workers died while trying to run it\n",
      "2025-08-09 01:18:17,196 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:49188' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('partition_elements_stacker-aggregate-b574a2d945351dee1dc425428ec07cad0', 0), ('partition_elements_stacker-aggregate-b574a2d945351dee1dc425428ec07cad0', 2)} (stimulus_id='handle-worker-cleanup-1754682497.1733463')\n",
      "2025-08-09 01:18:17,727 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "ename": "KilledWorker",
     "evalue": "Attempted to run task 'partition_elements_stacker-aggregate-finalize-hlgfinalizecompute-22ff46393343492ebf8fd4684d76bcb2' on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:49188. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKilledWorker\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m      7\u001b[0m     partition_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(partition_content)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m da\u001b[38;5;241m.\u001b[39mconcatenate(partition_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     10\u001b[0m bow_matrix \u001b[38;5;241m=\u001b[39m model_bag\u001b[38;5;241m.\u001b[39mmap(to_dask_array)\u001b[38;5;241m.\u001b[39mreduction(\n\u001b[0;32m     11\u001b[0m     perpartition\u001b[38;5;241m=\u001b[39mpartition_elements_stacker,\n\u001b[0;32m     12\u001b[0m     aggregate\u001b[38;5;241m=\u001b[39mpartition_elements_stacker\n\u001b[1;32m---> 13\u001b[0m )\u001b[38;5;241m.\u001b[39mcompute()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\base.py:373\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\base.py:681\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    678\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39moptimize()\n\u001b[0;32m    679\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(flatten(expr\u001b[38;5;241m.\u001b[39m__dask_keys__()))\n\u001b[1;32m--> 681\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(expr, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\distributed\\client.py:2417\u001b[0m, in \u001b[0;36mClient._gather\u001b[1;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[0;32m   2415\u001b[0m     exception \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mexception\n\u001b[0;32m   2416\u001b[0m     traceback \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mtraceback\n\u001b[1;32m-> 2417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2419\u001b[0m     bad_keys\u001b[38;5;241m.\u001b[39madd(key)\n",
      "\u001b[1;31mKilledWorker\u001b[0m: Attempted to run task 'partition_elements_stacker-aggregate-finalize-hlgfinalizecompute-22ff46393343492ebf8fd4684d76bcb2' on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:49188. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html."
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "model_bag = model_data.to_bag()\n",
    "def to_dask_array(vec):\n",
    "    return da.from_array(vec, chunks=500).reshape(1, -1)\n",
    "\n",
    "def partition_elements_stacker(partition_content):\n",
    "    partition_list = list(partition_content)\n",
    "    return da.concatenate(partition_list, axis=0)\n",
    "\n",
    "bow_matrix = model_bag.map(to_dask_array).reduction(\n",
    "    perpartition=partition_elements_stacker,\n",
    "    aggregate=partition_elements_stacker\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f0f261-98b9-4299-851c-5ffdff9d56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90681708-2748-4d05-8e18-5141a5e187d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install zarr\n",
    "\n",
    "bow_matrix.rechunk(500).to_zarr('bow_matrix.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7f1ab-8015-4c18-b89a-9290938a3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Convert to Dask DataFrame using top_100_tokens as column names\n",
    "bow_ddf = dd.from_dask_array(bow_matrix, columns=top_100_tokens)\n",
    "\n",
    "# Preview the first few rows\n",
    "bow_ddf.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43d754-f6f4-4bda-b616-3b68bd6cde06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
